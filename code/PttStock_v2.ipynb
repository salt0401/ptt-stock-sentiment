{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6mAXLBPU50kd","executionInfo":{"status":"ok","timestamp":1700923469437,"user_tz":-480,"elapsed":21867,"user":{"displayName":"蘇振賢","userId":"10404037240247104526"}},"outputId":"d327f528-e138-4cba-d7fe-d5d089059951"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","source":["### 爬蟲"],"metadata":{"id":"aZ5e5fZU8V0w"}},{"cell_type":"markdown","source":["爬蟲套件"],"metadata":{"id":"xBWedUBY8dGQ"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from datetime import datetime, timedelta\n","import pandas as pd\n","from concurrent.futures import ThreadPoolExecutor\n","import time\n","import progressbar"],"metadata":{"id":"wk7KdKFE8Xi1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["爬蟲main function"],"metadata":{"id":"XDH7hZhX8exL"}},{"cell_type":"code","source":["class PTTScraper:\n","    base_url = \"https://www.ptt.cc\"\n","\n","    def __init__(self, _board):\n","        self.base_url = PTTScraper.base_url\n","        self.url = self.base_url + f\"/bbs/{_board}/index.html\" #讓 URL變成特定的板\n","\n","    def get_post_content(self, post_url): #self = 特定版的網址; post = 文章的網址\n","        soup = PTTScraper.get_soup(self.base_url + post_url) #透過beautiful soup 把該文章轉換為soup物件\n","        content = soup.find(id='main-content').text\n","\n","        # 抓取推文\n","        pushes = soup.find_all('div', class_='push')\n","\n","        with ThreadPoolExecutor() as executor:\n","            push_list = list(executor.map(self.get_push, pushes)) #用get_push 把push（userid, date等等）的一些特徵也處理完\n","\n","        return content, push_list\n","\n","    def get_push(self, push):\n","        try:\n","            if push.find('span', class_='push-tag') is None:\n","                return dict()\n","            push_tag = push.find('span', class_='push-tag').text.strip()\n","            push_userid = push.find('span', class_='push-userid').text.strip()\n","            push_content = push.find('span', class_='push-content').text.strip().lstrip(\":\")\n","            push_ipdatetime = push.find('span', class_='push-ipdatetime').text.strip()\n","            push_dict = {\n","                \"Tag\": push_tag,\n","                \"Userid\": push_userid,\n","                \"Content\": push_content,\n","                \"Ipdatetime\": push_ipdatetime\n","            }\n","        except Exception as e:\n","            print(e)\n","        return push_dict\n","\n","    @staticmethod\n","    def get_soup(url): # url = 文章網址\n","        headers = {\n","            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n","                          \"Chrome/58.0.3029.110 Safari/537.3\", }\n","        cookies = {\"over18\": \"1\"}\n","        response = requests.get(url, headers=headers, cookies=cookies)\n","        return BeautifulSoup(response.text, 'html.parser') #返回一個 BeautifulSoup 物件，該物件包含網頁的解析結構，您可以使用 BeautifulSoup 提供的方法和屬性來尋找、提取和處理網頁中的數據。\n","\n","    def fetch_post(self, url):\n","        soup = PTTScraper.get_soup(self.base_url + url)\n","\n","        # Extract post information\n","        try:\n","            if soup.find(id='main-content') is not None:\n","                content = soup.find(id='main-content').text\n","                content = content.split('※ 發信站')[0]\n","            if soup.find(class_='article-meta-value') is not None:\n","                author = soup.find(class_='article-meta-value').text\n","                title = soup.find_all(class_='article-meta-value')[-2].text\n","                date_str = soup.find_all(class_='article-meta-value')[-1].text\n","                date = datetime.strptime(date_str, '%a %b %d %H:%M:%S %Y')\n","            else:\n","                author = None\n","                title = None\n","                date_str = None\n","                date = None\n","        except Exception as e:\n","            print(self.base_url + url)\n","            print(e)\n","        # Extract comments\n","        pushes = soup.find_all('div', class_='push')\n","\n","        with ThreadPoolExecutor() as executor:\n","            push_list = list(executor.map(self.get_push, pushes))\n","        return {'Title': title, 'Author': author, 'Date': date, 'Content': content,\n","                'Link': url, 'Pushes': push_list}\n","\n","    def get_data_current_page(self, soup=None, until_date=datetime.now(), *args,\n","                              max_posts=1000, links_num=0):\n","        reach = False\n","        until_date = until_date.replace(hour=0, minute=0, second=0, microsecond=0)\n","\n","        if soup is None:\n","            soup = PTTScraper.get_soup(self.url)\n","        links = []\n","        div_element = soup.find('div', {'class': 'r-list-sep'})\n","        if div_element is None:\n","            for entry in reversed(soup.select('.r-ent')):\n","                try:\n","                    title = entry.find(\"div\", \"title\").text.strip()\n","                    if entry.find(\"div\", \"title\").a is None:\n","                        continue\n","                    # print(title)\n","                    if len(args) == 2:\n","                        if not (args[0] in title and args[1] in title):\n","                            continue\n","                    elif len(args) == 1:\n","                        if args[0] not in title:\n","                            print(\"1\")\n","                            # continue\n","                    else:\n","                        pass\n","                    date = entry.select('.date')[0].text.strip()\n","\n","                    post_date = datetime.strptime(date, '%m/%d').replace(year=until_date.year)\n","                    # print(len(links))\n","                    if len(links) + links_num >= max_posts or post_date < until_date:\n","                        reach = True\n","                        break\n","                    links.append(entry.select('.title a')[0]['href'])\n","                except Exception as e:\n","                    print(e)\n","        else:\n","            previous_elements = [element for element in div_element.previous_siblings if\n","                                 element.name == 'div' and 'r-ent' in element.get('class', [])]\n","            for element in reversed(previous_elements):\n","\n","                # 找到標題和連結的元素\n","                title_link_element = element.find('a')\n","                if title_link_element:\n","                    # 取得標題和連結\n","                    title = title_link_element.text.strip()\n","                    if len(args) == 2:\n","                        if not (args[0] in title and args[1] in title):\n","                            continue\n","                    links.append(title_link_element.get('href'))\n","                date_element = element.find('div', {'class': 'date'})\n","                if date_element:\n","                    # 取得發文日期\n","                    date = date_element.text.strip()\n","                post_date = datetime.strptime(date, '%m/%d').replace(year=until_date.year)\n","                if len(links) + links_num >= max_posts or post_date < until_date:\n","                    reach = True\n","                    break\n","        if 'post_date' not in locals():\n","            return [], False, 0\n","        print(post_date)\n","        # print(len(links))\n","        with ThreadPoolExecutor() as executor:\n","            data = list(executor.map(self.fetch_post, links))\n","        return data, reach, len(links)\n","\n","    def get_data_until(self, until_date, *args, max_posts=1000):\n","        \"\"\"\n","        取得到 until_date 之後的所有文章\n","        :param until_date:  日期\n","        :param max_posts: 最多抓取文章\n","        :return: 文章串列\n","        \"\"\"\n","        data = []\n","        if not isinstance(until_date, datetime):\n","            date = datetime.strptime(until_date, '%m/%d').replace(year=datetime.now().year)\n","        else:\n","            date = until_date\n","        links_num = 0\n","        while True:\n","            soup = PTTScraper.get_soup(self.url)\n","            data_curr, date_end, num = self.get_data_current_page(soup, date, *args,\n","                                                                  max_posts=max_posts, links_num=links_num)\n","            data.extend(data_curr)\n","            #print(\"文章總篇數:\", len(data))\n","            if date_end:\n","                return data\n","            links_num += num\n","\n","            # 找到上一頁的連結\n","            prev_link = soup.find('a', string='‹ 上頁')['href']\n","            self.url = self.base_url + prev_link\n","        return data\n","\n","    def get_data_days_before(self, delta_days, *args, max_posts=1000):\n","        \"\"\"\n","        取得 delat_days 天之前的文章\n","        :param delta_days: 間隔天數\n","        :param max_posts: 最多回抓取幾篇PO文\n","        :return: 文章 list\n","        \"\"\"\n","        after_date = datetime.now() - timedelta(days=delta_days)\n","        # print(args)\n","        return self.get_data_until(after_date, *args, max_posts=max_posts)\n","\n","    def get_title_and_before_days(self, *args, delta_days, max_posts=1000):\n","        return self.get_data_days_before(delta_days, *args, max_posts=max_posts)"],"metadata":{"id":"TmZrygWy8YOT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["指令function"],"metadata":{"id":"v4iikHPc8lJA"}},{"cell_type":"code","source":["# 使用方式\n","if __name__ == \"__main__\":\n","    board = \"Stock\"\n","    scraper = PTTScraper(board)\n","    begin = time.time() #看執行時間用的，不重要\n","    #data = scraper.get_data_days_before(1)\n","    dataA = scraper.get_title_and_before_days(\"盤後\", \"[閒聊]\", delta_days=350, max_posts=1000)\n","    end = time.time()\n","    print(end - begin)\n","    if dataA is not None:\n","        dfA = pd.DataFrame(dataA)\n","        print(dfA)\n","    # print(pd.DataFrame(df.Pushes[1]))\n","\n","# 這裡 push 跟 content 事實上沒有分乾淨，但直接看Push貌似是沒問題的\n","if __name__ == \"__main__\":\n","    board = \"Stock\"\n","    scraper = PTTScraper(board)\n","    begin = time.time() #看執行時間用的，不重要\n","    #data = scraper.get_data_days_before(1)\n","    dataD = scraper.get_title_and_before_days(\"盤中\", \"[閒聊]\", delta_days=350, max_posts=1000)\n","    end = time.time()\n","    print(end - begin)\n","    if dataD is not None:\n","        dfD = pd.DataFrame(dataD)\n","        print(dfD)"],"metadata":{"id":"xr_w8QF68i8I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["過濾function"],"metadata":{"id":"CieA6DpP8q9L"}},{"cell_type":"code","source":["words_to_removeD = ['盤後閒聊']\n","words_to_removeA = ['盤中閒聊']\n","# Use boolean indexing to delete rows containing specified words\n","dfD = dfD[~dfD['Title'].str.contains('|'.join(words_to_removeD))]\n","dfA = dfA[~dfA['Title'].str.contains('|'.join(words_to_removeA))]\n","# Reset the index to reorganize it\n","dfD = dfD.reset_index(drop=True)\n","dfA = dfA.reset_index(drop=True)\n","# Display the modified DataFrame\n","#print(\"\\nDataFrame after deleting rows containing specified words:\")\n","print(\"這是盤後閒聊\")\n","print(dfA.head())\n","print(\"這是盤中閒聊\")\n","print(dfD.head())\n"],"metadata":{"id":"x1q0JtHS8qcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 斷詞套件"],"metadata":{"id":"7R-Usq9B8udH"}},{"cell_type":"markdown","source":["外部下載"],"metadata":{"id":"oWwd1RPj89uA"}},{"cell_type":"code","source":["!pip install -U ckiptagger[tf,gdown]\n","from ckiptagger import data_utils, construct_dictionary, WS, POS, NER"],"metadata":{"id":"C_HuDf2V8um8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install opencc-python-reimplemented"],"metadata":{"id":"bsTRvQEP_MBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["內部colab連接"],"metadata":{"id":"NBtiFBBn9ABv"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","ws = WS(\"/content/drive/MyDrive/專題code/data\", disable_cuda=False)\n","pos = POS(\"/content/drive/MyDrive/專題code/data\", disable_cuda=False)\n","ner = NER(\"/content/drive/MyDrive/專題code/data\", disable_cuda=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"eeir1ngL848c","executionInfo":{"status":"error","timestamp":1700924303874,"user_tz":-480,"elapsed":21662,"user":{"displayName":"嚴臨","userId":"02181525152372105951"}},"outputId":"bfbdd0e4-c840-4a16-d4fa-620c236e6407"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-2f7959d36d6b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/專題code/data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/專題code/data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/專題code/data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'WS' is not defined"]}]},{"cell_type":"markdown","source":["引入新字典及正負面分隔"],"metadata":{"id":"x8At-Q9L9CPa"}},{"cell_type":"code","source":["#引入字典\n","opinion = pd.read_excel('/content/drive/MyDrive/專題code/程式碼的家/opinion.xlsx')\n","opinion_positive = opinion[opinion['情緒分數'] > 0]\n","opinion_negative = opinion[opinion['情緒分數'] < 0]\n","opinion_negative"],"metadata":{"id":"kiQHlZ2X9MDx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 資料整理"],"metadata":{"id":"o3A6yIXN9ZJA"}},{"cell_type":"markdown","source":["日期篩取"],"metadata":{"id":"AZ2tOcjf9bnp"}},{"cell_type":"code","source":["after_date_list = []\n","after_date_list = dfA['Date']\n","after_date_list_clear = [timestamp.date() for timestamp in after_date_list]\n","after_date_list_clear = [date.strftime('%Y-%m-%d') for date in after_date_list_clear]\n","\n","middle_date_list = []\n","middle_date_list = dfD['Date']\n","middle_date_list_clear = [timestamp.date() for timestamp in middle_date_list]\n","middle_date_list_clear = [date.strftime('%Y-%m-%d') for date in middle_date_list_clear]\n","middle_date_list_clear"],"metadata":{"id":"EDSDu2wy9g9e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["建立分數表\n","row為日期\n","column為特徵"],"metadata":{"id":"Q25_Pxp-93ro"}},{"cell_type":"code","source":["if len(after_date_list_clear) > len(middle_date_list_clear):\n","  sentiment_df = pd.DataFrame(columns=['y_m_+','y_m_-','y_m_t','y_a_+','y_a_-','y_a_t'],index=after_date_list_clear)\n","else:\n","  sentiment_df = pd.DataFrame(columns=['y_m_+','y_m_-','y_m_t','y_a_+','y_a_-','y_a_t'],index=middle_date_list_clear)"],"metadata":{"id":"gR4FKDE19kYw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 分數計算"],"metadata":{"id":"qjK8XmBy9qp_"}},{"cell_type":"code","source":["from collections import Counter\n","from itertools import chain"],"metadata":{"id":"tZS6Dqbk91pU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def push_sentiment_calculator(Pushes,date,date_tommow):\n","  df2 = list(Pushes)\n","  sentiment_grade = 0\n","  Positive_grades = 0\n","  Negative_grades = 0\n","  contents_only = []\n","  contents_only = list(map(lambda d: d.get(\"Content\", \"N/A\"), df2))\n","  contents_only\n","\n","  word_sentence_list1 = ws(\n","      contents_only,\n","      sentence_segmentation = True, # To consider delimiters\n","      segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}, # This is the defualt set of delimiters\n","      # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n","      # coerce_dictionary = dictionary2, # words in this dictionary are forced\n","  )\n","  one_dimensional_wordlist = list(chain(*word_sentence_list1))\n","\n","  # 两个示例列表\n","  list_A = one_dimensional_wordlist\n","\n","  # 使用 Counter 对列表 A 进行计数\n","  counter_A = Counter(list_A)\n","\n","  for index, row in opinion_positive.iterrows():\n","    # 如果字词在 list_A 中\n","    if row['情緒字詞'] in list_A:\n","        # 将对应分数加入 sentiment_grade\n","        Positive_grades += row['情緒分數']\n","\n","  for index, row in opinion_negative.iterrows():\n","    if row['情緒字詞'] in list_A:\n","        # 将对应分数加入 sentiment_grade\n","        Negative_grades += row['情緒分數']\n","\n","  sentiment_grade = Positive_grades + Negative_grades\n","\n","  sentiment_df.loc[date_tommow, 'y_m_+'] = Positive_grades\n","  sentiment_df.loc[date_tommow, 'y_m_-'] = Negative_grades\n","  sentiment_df.loc[date_tommow, 'y_m_t'] = sentiment_grade\n","  sentiment_df.loc[date_tommow, 'y_a_+'] = Positive_grades\n","  sentiment_df.loc[date_tommow, 'y_a_-'] = Negative_grades\n","  sentiment_df.loc[date_tommow, 'y_a_t'] = sentiment_grade\n","\n","  print(sentiment_grade)\n","  #print(Negative_counts)\n","  return sentiment_grade"],"metadata":{"id":"EeLSH3VM9ktf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["執行情緒特徵表填入"],"metadata":{"id":"aPS0cLf6-Ffz"}},{"cell_type":"code","source":["for i in range(1,len(after_date_list_clear),1):\n","  push_sentiment_calculator(dfA['Pushes'][i],after_date_list_clear[i],after_date_list_clear[i-1])\n","for i in range(1,len(middle_date_list_clear),1):\n","  push_sentiment_calculator(dfD['Pushes'][i],middle_date_list_clear[i],middle_date_list_clear[i-1])"],"metadata":{"id":"QW0U8pOD-FHG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_df"],"metadata":{"id":"xPxYxoqd-LIB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"mjUOCBjlC3Pt"}}]}